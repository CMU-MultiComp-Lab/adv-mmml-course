<!DOCTYPE html>
<html>

  <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>11-877 AMML | Schedule</title>
  <meta name="description" content="11-877 - Advanced Topics in Multimodal Machine Learning - Carnegie Mellon University - Spring 2022
">

  <link rel="shortcut icon" href="/adv-mmml-course/assets/img/favicon.ico">

  <link rel="stylesheet" href="/adv-mmml-course/assets/css/main.css">
  <link rel="canonical" href="/adv-mmml-course/spring2024/schedule/">

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="http://localhost:4000/adv-mmml-course/spring2023/">11-877 AMML</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Schedule</h1>
    <h2 class="post-description"></h2>
  </header>

  <article class="post-content Schedule clearfix">
    <table class="table table-hover">
      <colgroup>
        <col style="width:5%">
        <col style="width:55%">
        <col style="width:40%">
      </colgroup>
      <thead class="thead-light">
        <tr>
          <th scope="col">Date</th>
          <th scope="col">Topics</th>
          <th scope="col">Readings</th>
        </tr>
      </thead>
      <tbody>
        <p>** Exact topics and schedule subject to change, based on student interests and course discussions. **</p>

<tr class="past">
    <th scope="row">1/16</th>
    
    <td>
        Week 1 <strong>Introduction</strong> <a href="lecture1-Introduction.pdf">[slides]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     Course syllabus and requirements
                  </li>
               
                  <li style="font-size:12px;">
                     Multimodal principles&#58; heterogeneity, connections, and interactions
                  </li>
               
                  <li style="font-size:12px;">
                     Multimodal technical challenges
                  </li>
               
                  <li style="font-size:12px;">
                     Multimodal research problems
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2209.03430">Foundations and Trends in Multimodal Machine Learning&#58; Principles, Challenges, and Open Questions</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/1705.09406">Multimodal Machine Learning&#58; A Survey and Taxonomy</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/1206.5538">Representation Learning&#58; A Review and New Perspectives</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">1/23</th>
    
    <td>
        Week 2 <strong>Foundation1: Dimensions of Heterogeneity</strong> <a href="11877_week2.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     What is a taxonomy of the dimensions in which modalities can be heterogeneous? What are intuitive definitions of each dimension of heterogeneity?
                  </li>
               
                  <li style="font-size:12px;">
                     Heterogeneity is also often seen in several other ML subfields (e.g., domain adaptation, domain shift, transfer learning, multitask learning, federated learning, etc). What are some similarities and differences between the notions of heterogeneity between MMML and these fields? Can definitions or methods in each area be adapted to benefit other research areas?
                  </li>
               
                  <li style="font-size:12px;">
                     How can we formalize these dimensions of heterogeneity, and subsequently estimate these measures to quantify the degree in which modalities are different?
                  </li>
               
                  <li style="font-size:12px;">
                     Heterogeneity in noise (e.g., due to sensor and system failures) is a relative understudied dimension. How can we reliably understand the unique noise topologies in modalities, to design more robust models?
                  </li>
               
                  <li style="font-size:12px;">
                     Modality heterogeneity often implies the design of specialized models capturing the unique properties of each modality. What are some tradeoffs in modality-specific vs modality-general models?
                  </li>
               
                  <li style="font-size:12px;">
                     Within each of the 6 multimodal challenges - representation, alignment, reasoning, generation, transference, quantification, how can the study of heterogeneity inform various modeling decisions? What problems could happen in practice if heterogeneity is not properly understood or modeled?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning&#58; Grids, Groups, Graphs, Geodesics, and Gauges</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/1804.08328">Taskonomy&#58; Disentangling Task Transfer Learning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/1905.07553">Which Tasks Should Be Learned Together in Multi-task Learning?</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2002.02923">Geometric Dataset Distances via Optimal Transport</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2302.08646">AutoFed&#58; Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/NTIRE/Brummer_Natural_Image_Noise_Dataset_CVPRW_2019_paper.html">Natural Image Noise Dataset</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/1711.02173">Synthetic and Natural Noise Both Break Neural Machine Translation</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">1/30</th>
    
    <td>
        Week 3 <strong>Foundation2: Multimodal Connections</strong> <a href="11877_week3.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     What are the reasons why modalities can be connected with each other? Come up with a taxonomy of various dimensions. Think along both statistical, data-driven dimensions and semantic, hypothesis or knowledge driven dimensions. How can we define estimators, where we can accurately quantify the presence of each type of connection given a dataset?
                  </li>
               
                  <li style="font-size:12px;">
                     Are connections always strong and one-to-one? Reflect on what could make some cross-modal connections stronger or weaker, including many-to-many connections, ambiguity, noises, or adversarial attacks. How can we adapt our learning methods to account for these imperfections?
                  </li>
               
                  <li style="font-size:12px;">
                     Given trained multimodal models, how can we understand or visualize the nature of connections captured by the model? What benchmarks should we design to probe the quality of learned connections?
                  </li>
               
                  <li style="font-size:12px;">
                     How can we better learn connections that happen at a very fine-grained and compositional level? Are there new inductive biases we might need to build into vision-language connection models?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2210.01936">When and why vision-language models behave like bags-of-words, and what to do about it?</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://link.springer.com/article/10.1007/s13735-019-00187-6">Characterization and classification of semantic image-text relations</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2005.10243">What Makes for Good Views for Contrastive Learning?</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.pdf">Relaxing Contrastiveness in Multimodal Representation Learning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2210.09304">Non-Contrastive Learning Meets Language-Image Pre-Training</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://www.emerald.com/insight/content/doi/10.1108/00220410310506303/full/pdf?title=a-taxonomy-of-relationships-between-images-and-text">A Taxonomy of Relationships between Images and Text</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hager_Best_of_Both_Worlds_Multimodal_Contrastive_Learning_With_Tabular_and_CVPR_2023_paper.pdf">Best of Both Worlds&#58; Multimodal Contrastive Learning with Tabular and Imaging Data</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2303.03323">CleanCLIP&#58; Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://chinese.wooster.edu/files/barthes.pdf">Image-Music-Text</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://journals.sagepub.com/doi/epdf/10.1177/1470357205055928">A System for Image–text Relations in New (and Old) Media</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">2/6</th>
    
    <td>
        Week 4 <strong>Foundation3: Multimodal Interactions</strong> <a href="11877_week4.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     What are the different ways in which modalities can interact with each other when used for prediction tasks? Think across both semantic and statistical perspectives. Can we formalize a taxonomy of such interactions, which will enable us to compare and contrast them more precisely? In fact, should we even try creating such a taxonomy?
                  </li>
               
                  <li style="font-size:12px;">
                     Can you think of ways modalities could interact with each other, even if there is no prediction task? How are modalities interacting during cross-modal translation? During multimodal generation?
                  </li>
               
                  <li style="font-size:12px;">
                     Linking back to last week’s discussion, are there cases where modalities are connected but do not interact? Or interact but are not connected? Can we design formal experiments to test either hypothesis?
                  </li>
               
                  <li style="font-size:12px;">
                     What mathematical or empirical frameworks can be used to formalize the meaning of interactions? How can we subsequently define estimators, where we can accurately quantify the presence of each type of interactions given a dataset?
                  </li>
               
                  <li style="font-size:12px;">
                     Some definitions (from the semantic category) typically require human interactions to detect and quantify interactions. What are some opportunities and limitations of using human judgment to analyze interactions? Can we potentially design estimators to automate the human labeling process?
                  </li>
               
                  <li style="font-size:12px;">
                     Can you think of ways to utilize large language models or other foundation models to enhance the learning process of multimodal interactions?
                  </li>
               
                  <li style="font-size:12px;">
                     How to utilize cognitive theory to design a framework that can be used to understand and learn the interactions between multiple modalities that human beings face everyday?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2205.09256.pdf">Training Vision-Language Transformers from Captions</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://dl.acm.org/doi/pdf/10.1145/319382.319398">Ten Myths of Multimodal Interaction</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2401.01862.pdf">A Vision Check-up for Language Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2303.07226.pdf">Scaling Vision-Language Models with Sparse Mixture of Experts</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2302.12247">Quantifying &amp; Modeling Multimodal Interactions&#58; An Information Decomposition Framework</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://aclanthology.org/2020.emnlp-main.62/">Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://www.sciencedirect.com/science/article/pii/S0167865513002584">Multimodal interaction&#58; A review</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://dl.acm.org/doi/abs/10.1145/1027933.1027957">When do we interact multimodally?&#58; cognitive load and multimodal communication patterns</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027715300858?casa_token=p9q2QcgTKB0AAAAA:g8Z9aww-Ca86HAXX3coq0GPemqVHltlcaFouEWQ7PWM6mZONTdawEkQS9_lQTdrZ0oS_KHMaRQ">A multimodal parallel architecture&#58; A cognitive framework for multimodal interactions</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/cs/0308002">Quantifying and Visualizing Attribute Interactions</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://www.sesp.org/files/The%20Moderator-Baron.pdf">The Moderator-Mediator Variable Distinction in Social Psychological Research&#58; Conceptual, Strategic, and Statistical Considerations</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">2/13</th>
    
    <td>
        Week 5 <strong>Multimodal LLMs1: Data, Pretraining, and Scaling Laws</strong> <a href="11877_week5.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     What types of multimodal data noise are typically present in multimodal datasets, and how can they negatively impact the performance of a model during training? Can you provide examples of multimodal data points that might be considered noisy? Furthermore, how might we develop estimators capable of distinguishing between noisy and noise-free multimodal data pairs? If you have unlimited fundings to use for data filtering and data cleaning, what would be the ideal way to clean the multimodal dataset?
                  </li>
               
                  <li style="font-size:12px;">
                     Given the demonstrated effectiveness of high-quality pretraining, as evidenced by projects like Mistral, imagine you have access to a large-scale, high-quality multimodal dataset for pre-training purposes. What types of generalization or additional capabilities might this enable the model to acquire compared to those trained on lower-quality data? Why do models trained with high-quality data obtain such abilities?
                  </li>
               
                  <li style="font-size:12px;">
                     Considering the diversity of model architectures available for multimodal generation, which architecture would be most suitable for scaling general multimodal generation tasks? Moreover, which model architecture is best equipped to learn complex multimodal interactions effectively?
                  </li>
               
                  <li style="font-size:12px;">
                     What are some pros and cons of treating data from all modalities equally (throwing them into a single large generative Transformer, after tokenizing the data)?
                  </li>
               
                  <li style="font-size:12px;">
                     If you were leading a multimodal foundation model project equipped with extensive resources, including a skilled team and significant GPU capabilities, what multimodal architecture and types of multimodal data would you prioritize for an initial pilot study?
                  </li>
               
                  <li style="font-size:12px;">
                     In exploring the scaling laws of multimodal models, different papers have different definitions for scaling law formulas. Which factors should be incorporated into the scaling law formula, and which among these do you believe is the most critical to consider?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2102.03334">ViLT&#58; Vision-and-Language Transformer Without Convolution or Region Supervision</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2301.03728">Scaling Laws for Generative Mixed-Modal Language Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/eacad5b8e67850f2b8dd33d87691d097-Paper-Conference.pdf">Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2010.14701.pdf">Scaling Laws for Autoregressive Generative Modeling</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2401.17377.pdf">Infini-gram&#58; Scaling Unbounded n-gram Language Models to a Trillion Tokens</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2311.18775.pdf">CoDi-2&#58; In-Context, Interleaved, and Interactive Any-to-Any Generation</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2208.05516.pdf">Quality Not Quantity&#58; On the Interaction between Datase Design and Robustness of CLIP</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2304.06939.pdf">Multimodal C4&#58; An Open, Billion-scale Corpus of Images Interleaved with Text</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2307.10350.pdf">Improving Multimodal Datasets with Image Captioning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2302.10035.pdf">Large-scale Multi-Modal Pre-trained Models&#58; A Comprehensive Survey</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2203.01311">High-Modality Multimodal Transformer&#58; Quantifying Modality &amp; Interaction Heterogeneity for High-Modality Representation Learning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/1908.07490">LXMERT&#58; Learning Cross-Modality Encoder Representations from Transformers</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2307.05222.pdf">Generative Pretraining in Multimodality</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2102.01293">Scaling Laws for Transfer</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">2/20</th>
    
    <td>
        Week 6 <strong>Multimodal LLMs2: Fine-tuning, Instructing, Aligning, Model Merging</strong> <a href="11877_week6.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     Ensuring the effectiveness of multimodal foundation models through high-quality instruction tuning is vital. A study detailed at <a href="https://arxiv.org/pdf/2402.04333.pdf">here</a>  introduces a strategy for selecting significant data specifically suited for enhancing instruction tuning for language models. A primary challenge in this approach is determining which data are most crucial for targeted instruction tuning. How can we accurately identify and select the most impactful data for enhancing instruction tuning in multimodal foundation models? Given the complexity of diverse and multimodal information, what strategies can ensure the effectiveness of instruction tuning data for specific tasks?
                  </li>
               
                  <li style="font-size:12px;">
                     For model merging, mixture-of-expert-based models enable a new paradigm to utilize multiple expert models for specific tasks. <a href="https://arxiv.org/pdf/2402.05859.pdf">Here</a> shows a promising method to utilize multiple models together. When it comes to multimodal tasks, how might we design a similar system for multimodal tasks that have human-level intelligence? What methodologies could enable the integration of various multimodal models to perform complex tasks such as social interaction effectively?
                  </li>
               
                  <li style="font-size:12px;">
                     What is the intuition of utilizing frozen large language models as the backbone for multimodal tasks? Which types of encoders would facilitate the integration of diverse information into a format understandable by LLMs? How do these LLMs process and interpret information from different modalities?
                  </li>
               
                  <li style="font-size:12px;">
                     Considering the various methods available for LLM alignment, is aligning multimodal models perceived to be more challenging or easier? What factors contribute to the difficulty of multimodal alignment, and how might this be related to those previously discussed fundamental parts of multimodal machine learning like interaction and connection?
                  </li>
               
                  <li style="font-size:12px;">
                     How can we categorize the taxonomy of general AI alignment? Can we classify the AI alignment categories based on the goal of conducting alignment? Assuming the existence of an oracle alignment method, what behaviors would we expect from an aligned AI model? Please list some behaviors that should be exhibited by AI following successful alignment.
                  </li>
               
                  <li style="font-size:12px;">
                     What is the taxonomy of general AI alignment? Can we classify based on the goal of alignment? Imagine we have an oracle alignment method, what kind of behavior we expect the model to have after alignment? Please list some of the expected behavior that AI should have after alignment.
                  </li>
               
                  <li style="font-size:12px;">
                     What distinguishes AI alignment from AI personalization? When focusing on AI alignment and personalization, what are the key differences and considerations to keep in mind? Is personalization an easier or harder thing to conduct compared with alignment?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2102.01293">Case Law Grounding&#58; Aligning Judgments of Humans and AI on Socially-Constructed Concepts</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2309.14525.pdf">Aligning AI with shared human values</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2310.07019.pdf">Improved Baselines with Visual Instruction Tuning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2008.02275.pdf">Value Alignment for Advanced Artificial Judicial Intelligence</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4252645"></a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2310.03744.pdf">Improved Baselines with Visual Instruction Tunin</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2106.13884.pdf">Multimodal Few-Shot Learning with Frozen Language Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2301.12597">BLIP-2&#58; Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2310.09478">MiniGPT-v2&#58; large language model as a unified interface for vision-language multi-task learning</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2304.14933.pdf">An Empirical Study of Multimodal Model Merging</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.mlr.press/v202/wu23t/wu23t.pdf">π-Tuning&#58; Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2311.10122.pdf">Video-LLaVA&#58; Learning United Visual Representation by Alignment Before Projection</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2303.16200.pdf?trk=public_post_comment-text">Natural Selection&#58; Favors AIs over Humans</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/702f4db7543a7432431df588d57bc7c9-Paper-Conference.pdf">Mind the Gap&#58; Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">2/27</th>
    
    <td>
        Week 7 <strong>Multimodal LLMs3: Generative Models and LLMs</strong> <a href="11877_week7.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     Connecting with multimodal foundation topics discussed in the previous week, what types of multimodal interactions or connections are large-scale generative models learning to capture? How to link multimodal interactions with generative AI architecture? How to combine mathematical theory related to multimodal interactions to design the next generation of generative AI architecture?
                  </li>
               
                  <li style="font-size:12px;">
                     With the advancement of generative AI, distinguishing between AI-generated and human-created content is becoming increasingly challenging. Besides watermarking, which has its limitations, are there other effective methods to differentiate between AI-generated and human-created content across various modalities (text, audio, video, image)? Or is it becoming virtually impossible to make this distinction?
                  </li>
               
                  <li style="font-size:12px;">
                     What is the taxonomy of safety issues, social impact, and ethical concerns associated with generative AI development? How should we update best practices to address these ethical concerns? Who should initiate and lead this dialogue? What steps can be taken to mitigate these ethical issues effectively? Imagine we have an oracle multimodal generative AI system that is used on a large scale. What types of data pollution would it have if most of its data were published on the Internet?
                  </li>
               
                  <li style="font-size:12px;">
                     When assessing the quality of multimodal outputs from generative AI systems, which dimensions should be prioritized? Can we develop metrics that allow for large-scale evaluation while mitigating potential safety and ethical risks?
                  </li>
               
                  <li style="font-size:12px;">
                     Diffusion models have shown remarkable performance in controllable text-to-image generation. Could you explain the intuition behind why diffusion models are effective, especially in comparison to other generative AI models like GANs/ VAEs / AR-based LLMs? Some works claim that scaling up GANs can beat diffusion models (<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">here</a>) and some work claims that language models are better than diffusion models for image generation (<a href="https://arxiv.org/pdf/2310.05737.pdf">here</a>). Which generative model family do you think is the most promising one for multimodal generation?
                  </li>
               
                  <li style="font-size:12px;">
                     For state-of-the-art video generation models like Sora, Yann Lecun mentioned in <a href="https://twitter.com/ylecun/status/1758740106955952191">here</a> that Sora does not understand the real world and its corresponding physical rules. Do you agree with this view? Can the future development of generative AI systems truly incorporate real-world knowledge, or are they limited in this aspect? Is pursuing generative AI a viable path towards achieving Artificial General Intelligence (AGI)?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/43a69d143273bd8215578bde887bb552-Paper-Conference.pdf">Generating Images with Multimodal Language Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/dd83eada2c3c74db3c7fe1c087513756-Paper-Datasets_and_Benchmarks.pdf">Holistic Evaluation of Text-to-Image Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openai.com/research/video-generation-models-as-world-simulators">Video generation models as world simulators</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2306.01953.pdf">Invisible Image Watermarks Are Provably Removable Using Generative AI</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2312.14125.pdf">VideoPoet&#58; A Large Language Model for Zero-Shot Video Generation</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2312.17172.pdf">Unified-IO 2&#58; Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/971f1e59cd956cc094da4e2f78c6ea7c-Paper-Conference.pdf">StableRep&#58; Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/html/2401.01335v1">Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2308.14752.pdf">AI Deception&#58; A Survey of Examples, Risks, and Potential Solutions</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2306.05949.pdf">Evaluating the Social Impact of Generative AI Systems in Systems and Society</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://www.sciencedirect.com/science/article/pii/S0160791X2300177X">Generative AI&#58; Here to stay, but for good?</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://www.tandfonline.com/doi/abs/10.1207/s15326985ep2704_8">Generative Learning Processes of the Brain</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/8171ac2c5544a5cb54ac0f38bf477af4-Paper.pdf">VAEM&#58; a Deep Generative Model for Heterogeneous Mixed Type Data</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2305.11846.pdf">Any-to-Any Generation via Composable Diffusion</a> <br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2203.02923.pdf">GeoDiff&#58; A Geometric Diffusion Model For Molecular rConformation Generation</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">3/5</th>
    
    <td>
        Week 8 <strong>No classes – Spring break</strong>
 <br />
            <ul>
               
        </ul>
    </td>
    <td>
        <ul>
               
        </ul>
    </td>
    
</tr>

<tr class="past">
    <th scope="row">3/12</th>
    
    <td>
        Week 9 <strong>Interaction1: Reasoning and Large Models</strong> <a href="*.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     Currently, most reasoning models are basically limited to vision and language domains. However, in our real world, multimodal reasoning more broadly exists and has more diverse forms. Can you list a few more examples of multimodal reasoning tasks in our daily life that rely on other modalities and how symbolic or unique reasoning methods can be applied to them?
                  </li>
               
                  <li style="font-size:12px;">
                     Can you create a taxonomy of all potential symbolic systems that can be helpful for different types of multimodal reasoning tasks like AMR graphs, knowledge graphs and programs? What are their unique advantages and disadvantages?
                  </li>
               
                  <li style="font-size:12px;">
                     Based on https://openreview.net/pdf?id=GPKTIktA0k, are there any other complex reasoning tasks besides reverse logic problems that you think the current foundation models might not handle well? How can neural symbolic models be incorporated to help with those hard cases?
                  </li>
               
                  <li style="font-size:12px;">
                     Besides https://arxiv.org/pdf/2402.03268.pdf, can you imagine any other potential way to uncover the reasoning capabilities of black-box models, such as large language models and other multimodal foundation models? How can one discover specifically the cross-modal reasoning processes in such a black-box model?
                  </li>
               
                  <li style="font-size:12px;">
                     To what extent do we need external knowledge when performing reasoning, specifically multimodal reasoning? What type of external knowledge is likely to be needed to succeed in multimodal reasoning?
                  </li>
               
                  <li style="font-size:12px;">
                     What are the main advantages of reasoning-based approaches, when compared to large-scale multimodal models discussed in the previous lectures? What are the potential issues with reasoning? Can we perform reasoning on very large datasets? Why do pre-training methods eventually learn reasoning processes similar to humans? Or will we still need human and domain knowledge to some extent?
                  </li>
               
                  <li style="font-size:12px;">
                     Are there unique technical challenges that arise when we consider utilizing neural symbolic methods on multimodal data as performed on multimodal data? What are these unique challenges? How can we start studying these challenges in future research?
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2305.10601.pdf">Tree of Thoughts&#58; Deliberate Problem Solving with Large Language Models</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2303.08128.pdf">ViperGPT&#58; Visual Inference via Python Execution for Reasoning</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2402.03268.pdf">Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2205.09712.pdf">Selection-Inference&#58; Exploiting Large Language Models for Interpretable Logical Reasoning</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/abs/2210.15037">Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2210.00312.pdf">Multimodal Analogical Reasoning Over Knowledge Graphs</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/1904.12584.pdf">The Neuro-Symbolic Concept Learner&#58; Interpreting Scenes, Words, and Sentences From Natural Supervision</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Neural_Scene_De-Rendering_CVPR_2017_paper.pdf">Neural Scene De-rendering</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://link.springer.com/article/10.1007/s10462-023-10448-w">Neurosymbolic AI&#58; the 3rd wave</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_VQA-GNN_Reasoning_with_Multimodal_Knowledge_via_Graph_Neural_Networks_for_ICCV_2023_paper.pdf">VQA-GNN&#58; Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://arxiv.org/pdf/2304.07633.pdf">Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf">Neural Module Networks</a><br />
                  </li>
               
                  <li style="font-size:12px;">
                     <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/894">Logical Versus Analogical or Symbolic Versus Connectionist or Neat Versus Scruffy</a><br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="warning">
    <th scope="row">3/19</th>
    
    <td>
        Week 10 <strong>Interaction2: Embodied AI</strong> <a href="*.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     TBD
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="">TBD</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="upcoming">
    <th scope="row">3/26</th>
    
    <td>
        Week 11 <strong>Interaction3: Pragmatics and Human-in-the-loop</strong> <a href="*.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     TBD
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="">TBD</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="upcoming">
    <th scope="row">4/2</th>
    
    <td>
        Week 12 <strong>Ethics and Safety</strong> <a href="*.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     TBD
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="">TBD</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="upcoming">
    <th scope="row">4/9</th>
    
    <td>
        Week 13 <strong>Efficiency</strong> <a href="*.pdf">[synopsis]</a>
 <br />
            <ul>
               
                  <li style="font-size:12px;">
                     TBD
                  </li>
               
        </ul>
    </td>
    <td>
        <ul>
               
                  <li style="font-size:12px;">
                     <a href="">TBD</a> <br />
                  </li>
               
        </ul>
    </td>
    
</tr>

<tr class="upcoming">
    <th scope="row">4/16</th>
    
    <td>
        Week 14 <strong>Open Discussion</strong>
 <br />
            <ul>
               
        </ul>
    </td>
    <td>
        <ul>
               
        </ul>
    </td>
    
</tr>

<tr class="upcoming">
    <th scope="row">4/23</th>
    
    <td>
        Week 15 <strong>Project presentations</strong>
 <br />
            <ul>
               
        </ul>
    </td>
    <td>
        <ul>
               
        </ul>
    </td>
    
</tr>


      </tbody>
    </table>
  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Advanced Topics in Multimodal Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1" style="width:200px;">
        <ul class="contact-list">
          <li class="p-name">CMU MultiComp Lab</li></ul>
      </div>

      <div class="footer-col footer-col-2" style="width:200px;"><ul class="social-media-list"><li><a href="https://github.com/CMU-MultiComp-Lab" target="_blank"><i class="fab fa-github"></i> <span class="username">CMU-MultiComp-Lab</span></a></li><li><a href="https://www.youtube.com/channel/UCqlHIJTGYhiwQpNuPU5e2gg"  target="_blank"><i class="fab fa-youtube"></i> <span class="username">YouTube</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2024 Carnegie Mellon University. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/adv-mmml-course/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.js"></script>
<script src="/adv-mmml-course/assets/js/katex.js"></script>



<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'hover';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Adjust LaTeX JS -->
<script src="/adv-mmml-course/assets/js/latex.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/adv-mmml-course/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/adv-mmml-course/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-131744305-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
