- date: 1/16
  title: >
    Week 1: <strong>Introduction</strong> <a href="lecture1-Introduction.pdf">[slides]</a>

  slides:
  topics:
    - Course syllabus and requirements
    - Multimodal principles&#58; heterogeneity, connections, and interactions
    - Multimodal technical challenges
    - Multimodal research problems
  readings:
    - <a href="https://arxiv.org/abs/2209.03430">Foundations and Trends in Multimodal Machine Learning&#58; Principles, Challenges, and Open Questions</a> <br/>
    - <a href="https://arxiv.org/abs/1705.09406">Multimodal Machine Learning&#58; A Survey and Taxonomy</a> <br/>
    - <a href="https://arxiv.org/abs/1206.5538">Representation Learning&#58; A Review and New Perspectives</a> <br/>

- date: 1/23
  title: >
    Week 2: <strong>Foundation1: Dimensions of Heterogeneity</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - What is a taxonomy of the dimensions in which modalities can be heterogeneous? What are intuitive definitions of each dimension of heterogeneity?
    - Heterogeneity is also often seen in several other ML subfields (e.g., domain adaptation, domain shift, transfer learning, multitask learning, federated learning, etc). What are some similarities and differences between the notions of heterogeneity between MMML and these fields? Can definitions or methods in each area be adapted to benefit other research areas?
    - How can we formalize these dimensions of heterogeneity, and subsequently estimate these measures to quantify the degree in which modalities are different?
    - Heterogeneity in noise (e.g., due to sensor and system failures) is a relative understudied dimension. How can we reliably understand the unique noise topologies in modalities, to design more robust models?
    - Modality heterogeneity often implies the design of specialized models capturing the unique properties of each modality. What are some tradeoffs in modality-specific vs modality-general models?
    - Within each of the 6 multimodal challenges - representation, alignment, reasoning, generation, transference, quantification, how can the study of heterogeneity inform various modeling decisions? What problems could happen in practice if heterogeneity is not properly understood or modeled?
  readings:
    - <a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning#58; Grids, Groups, Graphs, Geodesics, and Gauges</a> <br/>
    - <a href="https://arxiv.org/abs/1804.08328">Taskonomy#58; Disentangling Task Transfer Learning</a> <br/>
    - <a href="https://arxiv.org/abs/1905.07553">Which Tasks Should Be Learned Together in Multi-task Learning?</a> <br/>
    - <a href="https://arxiv.org/abs/2002.02923">Geometric Dataset Distances via Optimal Transport</a> <br/>
    - <a href="https://arxiv.org/abs/2302.08646">AutoFed#58; Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving</a> <br/>
    - <a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/NTIRE/Brummer_Natural_Image_Noise_Dataset_CVPRW_2019_paper.html">Natural Image Noise Dataset</a> <br/>
    - <a href="https://arxiv.org/abs/1711.02173">Synthetic and Natural Noise Both Break Neural Machine Translation</a> <br/>

- date: 1/30
  title: >
    Week 3: <strong>Foundation2: Multimodal Connections</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - What are the reasons why modalities can be connected with each other? Come up with a taxonomy of various dimensions. Think along both statistical, data-driven dimensions and semantic, hypothesis or knowledge driven dimensions. How can we define estimators, where we can accurately quantify the presence of each type of connection given a dataset?
    - Are connections always strong and one-to-one? Reflect on what could make some cross-modal connections stronger or weaker, including many-to-many connections, ambiguity, noises, or adversarial attacks. How can we adapt our learning methods to account for these imperfections?
    - Given trained multimodal models, how can we understand or visualize the nature of connections captured by the model? What benchmarks should we design to probe the quality of learned connections?
    - How can we better learn connections that happen at a very fine-grained and compositional level? Are there new inductive biases we might need to build into vision-language connection models?
  readings:
    - <a href="https://arxiv.org/abs/2210.01936">When and why vision-language models behave like bags-of-words, and what to do about it?</a> <br/>
    - <a href="https://link.springer.com/article/10.1007/s13735-019-00187-6">Characterization and classification of semantic image-text relations</a> <br/>
    - <a href="https://arxiv.org/abs/2005.10243">What Makes for Good Views for Contrastive Learning?</a> <br/>
    - <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.pdf">Relaxing Contrastiveness in Multimodal Representation Learning</a> <br/>
    - <a href="https://arxiv.org/abs/2210.09304">Non-Contrastive Learning Meets Language-Image Pre-Training</a> <br/>
    - <a href="https://www.emerald.com/insight/content/doi/10.1108/00220410310506303/full/pdf?title=a-taxonomy-of-relationships-between-images-and-text">A Taxonomy of Relationships between Images and Text</a> <br/>
    - <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hager_Best_of_Both_Worlds_Multimodal_Contrastive_Learning_With_Tabular_and_CVPR_2023_paper.pdf">Best of Both Worlds#58; Multimodal Contrastive Learning with Tabular and Imaging Data</a> <br/>
    - <a href="https://arxiv.org/abs/2303.03323">CleanCLIP#58; Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</a> <br/>
    - <a href="https://chinese.wooster.edu/files/barthes.pdf">Image-Music-Text</a> <br/>
    - <a href="https://journals.sagepub.com/doi/epdf/10.1177/1470357205055928">A System for Image–text Relations in New (and Old) Media</a> <br/>

- date: 2/6
  title: >
    Week 4: <strong>Foundation2: Multimodal Interactions</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - What are the different ways in which modalities can interact with each other when used for prediction tasks? Think across both semantic and statistical perspectives. Can we formalize a taxonomy of such interactions, which will enable us to compare and contrast them more precisely? In fact, should we even try creating such a taxonomy?
    - Can you think of ways modalities could interact with each other, even if there is no prediction task? How are modalities interacting during cross-modal translation? During multimodal generation?
    - Linking back to last week’s discussion, are there cases where modalities are connected but do not interact? Or interact but are not connected? Can we design formal experiments to test either hypothesis?
    - What mathematical or empirical frameworks can be used to formalize the meaning of interactions? How can we subsequently define estimators, where we can accurately quantify the presence of each type of interactions given a dataset?
    - Some definitions (from the semantic category) typically require human interactions to detect and quantify interactions. What are some opportunities and limitations of using human judgment to analyze interactions? Can we potentially design estimators to automate the human labeling process?
    - Can you think of ways to utilize large language models or other foundation models to enhance the learning process of multimodal interactions?
    - How to utilize cognitive theory to design a framework that can be used to understand and learn the interactions between multiple modalities that human beings face everyday?
  readings:
    - <a href="https://arxiv.org/pdf/2205.09256.pdf">Training Vision-Language Transformers from Captions</a> <br/>
    - <a href="https://dl.acm.org/doi/pdf/10.1145/319382.319398">Ten Myths of Multimodal Interaction</a> <br/>
    - <a href="https://arxiv.org/pdf/2401.01862.pdf">A Vision Check-up for Language Models</a> <br/>
    - <a href="https://arxiv.org/pdf/2303.07226.pdf">Scaling Vision-Language Models with Sparse Mixture of Experts</a> <br/>
    - <a href="https://arxiv.org/abs/2302.12247">Quantifying & Modeling Multimodal Interactions#58; An Information Decomposition Framework</a> <br/>
    - <a href="https://aclanthology.org/2020.emnlp-main.62/">Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/pii/S0167865513002584">Multimodal interaction#58; A review</a> <br/>
    - <a href="https://dl.acm.org/doi/abs/10.1145/1027933.1027957">When do we interact multimodally?#58; cognitive load and multimodal communication patterns</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027715300858?casa_token=p9q2QcgTKB0AAAAA:g8Z9aww-Ca86HAXX3coq0GPemqVHltlcaFouEWQ7PWM6mZONTdawEkQS9_lQTdrZ0oS_KHMaRQ">A multimodal parallel architecture#58; A cognitive framework for multimodal interactions</a> <br/>
    - <a href="https://arxiv.org/abs/cs/0308002">Quantifying and Visualizing Attribute Interactions</a> <br/>
    - <a href="https://www.sesp.org/files/The%20Moderator-Baron.pdf">The Moderator-Mediator Variable Distinction in Social Psychological Research#58; Conceptual, Strategic, and Statistical Considerations</a> <br/>

- date: 2/13
  title: >
    Week 5: <strong>Multimodal LLMs1: Data, Pretraining, and Scaling Laws</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 2/20
  title: >
    Week 6: <strong>Multimodal LLMs2: Fine-tuning, Instructing, Aligning, Model Merging</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 2/27
  title: >
    Week 7: <strong>Multimodal LLMs3: Generative Models and LLMs</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 3/5
  title: >
    Week 8: <strong>No classes – Spring break</strong>
  topics:
    - None!
  readings:
    - None!

- date: 3/12
  title: >
    Week 9: <strong>Interaction1: Neurosymbolic Reasoning</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 3/19
  title: >
    Week 10: <strong>Interaction2: Embodied AI</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 3/26
  title: >
    Week 11: <strong>Interaction3: Pragmatics and Human-in-the-loop</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 4/2
  title: >
    Week 12: <strong>Ethics and Safety</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 4/9
  title: >
    Week 12: <strong>Efficiency</strong> <a href="*.pdf">[synopsis]</a>
  slides:
  topics:
    - TBD
  readings:
    - <a href="">TBD</a> <br/>

- date: 4/16
  title: >
    Week 14: <strong>Open Discussion</strong>
  topics:
  
  readings:

- date: 4/23
  title: >
    Week 15: <strong>Project presentations</strong>
  topics:
  
  readings: